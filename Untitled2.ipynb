{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5rThlKMMA-eH",
        "GFosWWkvAsVm"
      ],
      "authorship_tag": "ABX9TyPnB1x3vy6BCuOT6Ls6jwvv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgariv/mask-dataset/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rThlKMMA-eH"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0JALXHFDMjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18611bb-fbf3-4835-a75a-16a988e0c97e"
      },
      "source": [
        "%%bash\r\n",
        "git clone https://www.github.com/tensorflow/models.git\r\n",
        "git clone https://www.github.com/lgariv/mask-dataset.git\r\n",
        "mv mask-dataset/ dataset/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "warning: redirecting to https://github.com/tensorflow/models.git/\n",
            "Cloning into 'mask-dataset'...\n",
            "warning: redirecting to https://github.com/lgariv/mask-dataset.git/\n",
            "Checking out files:  73% (2504/3415)   \rChecking out files:  74% (2528/3415)   \rChecking out files:  75% (2562/3415)   \rChecking out files:  76% (2596/3415)   \rChecking out files:  77% (2630/3415)   \rChecking out files:  78% (2664/3415)   \rChecking out files:  79% (2698/3415)   \rChecking out files:  80% (2732/3415)   \rChecking out files:  81% (2767/3415)   \rChecking out files:  82% (2801/3415)   \rChecking out files:  83% (2835/3415)   \rChecking out files:  84% (2869/3415)   \rChecking out files:  85% (2903/3415)   \rChecking out files:  86% (2937/3415)   \rChecking out files:  87% (2972/3415)   \rChecking out files:  88% (3006/3415)   \rChecking out files:  89% (3040/3415)   \rChecking out files:  90% (3074/3415)   \rChecking out files:  91% (3108/3415)   \rChecking out files:  91% (3134/3415)   \rChecking out files:  92% (3142/3415)   \rChecking out files:  93% (3176/3415)   \rChecking out files:  94% (3211/3415)   \rChecking out files:  95% (3245/3415)   \rChecking out files:  96% (3279/3415)   \rChecking out files:  97% (3313/3415)   \rChecking out files:  98% (3347/3415)   \rChecking out files:  99% (3381/3415)   \rChecking out files: 100% (3415/3415)   \rChecking out files: 100% (3415/3415), done.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPjnm7Uj38ET"
      },
      "source": [
        "%%bash\r\n",
        "cd /content/models/research\r\n",
        "protoc --python_out=. object_detection/protos/*.proto\r\n",
        "python -q setup.py build\r\n",
        "python -q setup.py install\r\n",
        "cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNwnZLZYMxoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc16b0b-75dd-488b-8be1-6c0a492b9472"
      },
      "source": [
        "%%bash\r\n",
        "python /content/dataset/generate_tfrecord.py --csv_input=/content/dataset/images/train_labels.csv --image_dir=/content/dataset/images/train --output_path=/content/dataset/images/train.record\r\n",
        "python /content/dataset/generate_tfrecord.py --csv_input=/content/dataset/images/test_labels.csv --image_dir=/content/dataset/images/test --output_path=/content/dataset/images/test.record"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the TFRecords: /content/dataset/images/train.record\n",
            "Successfully created the TFRecords: /content/dataset/images/test.record\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-18 04:25:51.173933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-18 04:25:54.967223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFosWWkvAsVm"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3tlIpN12cOt"
      },
      "source": [
        "# import the necessary packages\r\n",
        "import os\r\n",
        "\r\n",
        "# define the base path to the input dataset and then use it to derive\r\n",
        "# the path to the images directory and annotation CSV file\r\n",
        "BASE_PATH = \"/content/dataset/images\"\r\n",
        "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"train\"])\r\n",
        "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"train_labels.csv\"])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu7JQW8J6IIf"
      },
      "source": [
        "# define the path to the base output directory\r\n",
        "BASE_OUTPUT = \"/content/output\"\r\n",
        "!rm -rf /content/output\r\n",
        "!mkdir /content/output\r\n",
        "\r\n",
        "# define the path to the output serialized model, model training plot,\r\n",
        "# and testing image filenames\r\n",
        "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.h5\"])\r\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\r\n",
        "TEST_FILENAMES = os.path.sep.join([BASE_OUTPUT, \"test_images.txt\"])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_JplYlC6U5V"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\r\n",
        "from tensorflow.keras.preprocessing.image import load_img\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import os"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8jTyTzk6frl"
      },
      "source": [
        "# load the contents of the CSV annotations file\r\n",
        "print(\"[INFO] loading dataset...\")\r\n",
        "rows = open(ANNOTS_PATH).read().strip().split(\"\\n\")\r\n",
        "print(rows)\r\n",
        "rows.pop(0)\r\n",
        "\r\n",
        "# initialize the list of data (images), our target output predictions\r\n",
        "# (bounding box coordinates), along with the filenames of the\r\n",
        "# individual images\r\n",
        "data = []\r\n",
        "labels = []\r\n",
        "bboxes = []\r\n",
        "imagePaths = []\r\n",
        "\r\n",
        "r\"\"\"\r\n",
        "mask_data = []\r\n",
        "mask_targets = []\r\n",
        "mask_filenames = []\r\n",
        "\r\n",
        "no_mask_data = []\r\n",
        "no_mask_targets = []\r\n",
        "no_mask_filenames = []\r\n",
        "\r\n",
        "incorrect_data = []\r\n",
        "incorrect_targets = []\r\n",
        "incorrect_filenames = []\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOyzIEHz6vPd"
      },
      "source": [
        "# loop over the rows\r\n",
        "for row in rows:\r\n",
        "\t# break the row into the filename and bounding box coordinates\r\n",
        "  row = row.split(\",\")\r\n",
        "  (filename, startX, startY, endX, endY, label) = [row[0], row[4], row[5], row[6], row[7], row[3]]\r\n",
        "\r\n",
        "  r\"\"\"\r\n",
        "  if label == 'with_mask':\r\n",
        "  elif label == 'without_mask':\r\n",
        "  else:\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # derive the path to the input image, load the image (in OpenCV\r\n",
        "\t# format), and grab its dimensions\r\n",
        "  (w, h) = [float(row[1]), float(row[2])]\r\n",
        "\r\n",
        "\t# scale the bounding box coordinates relative to the spatial\r\n",
        "\t# dimensions of the input image\r\n",
        "  startX = float(startX) / w\r\n",
        "  startY = float(startY) / h\r\n",
        "  endX = float(endX) / w\r\n",
        "  endY = float(endY) / h\r\n",
        "\r\n",
        "  # load the image and preprocess it\r\n",
        "  image = load_img(os.path.sep.join(['/content/dataset/images/train', filename]), target_size=(224, 224))\r\n",
        "  image = img_to_array(image)\r\n",
        "\r\n",
        "  imagePath = os.path.sep.join(['/content/dataset/images/train', filename])\r\n",
        "\r\n",
        "  data.append(image)\r\n",
        "  labels.append(label)\r\n",
        "  bboxes.append((startX, startY, endX, endY))\r\n",
        "  imagePaths.append(imagePath)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PZV5DQb7UCY"
      },
      "source": [
        "# convert the data, class labels, bounding boxes, and image paths to\r\n",
        "# NumPy arrays, scaling the input pixel intensities from the range\r\n",
        "# [0, 255] to [0, 1]\r\n",
        "data = np.array(data, dtype=\"float32\") / 255.0\r\n",
        "labels = np.array(labels)\r\n",
        "bboxes = np.array(bboxes, dtype=\"float32\")\r\n",
        "imagePaths = np.array(imagePaths)\r\n",
        "\r\n",
        "# perform one-hot encoding on the labels\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "lb = LabelBinarizer()\r\n",
        "labels = lb.fit_transform(labels)\r\n",
        "\r\n",
        "# only there are only two labels in the dataset, then we need to use\r\n",
        "# Keras/TensorFlow's utility function as well\r\n",
        "if len(lb.classes_) == 2:\r\n",
        "\tlabels = to_categorical(labels)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmw3rJP-l_mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209a7ac5-7d4c-4a88-8e75-6f3519cc895e"
      },
      "source": [
        "# partition the data into training and testing splits using 80% of\r\n",
        "# the data for training and the remaining 20% for testing\r\n",
        "split = train_test_split(data, labels, bboxes, imagePaths,\r\n",
        "\ttest_size=0.20, random_state=42)\r\n",
        "\r\n",
        "# unpack the data split\r\n",
        "(trainImages, testImages) = split[:2]\r\n",
        "(trainLabels, testLabels) = split[2:4]\r\n",
        "(trainBBoxes, testBBoxes) = split[4:6]\r\n",
        "(trainPaths, testPaths) = split[6:]\r\n",
        "\r\n",
        "# write the testing image paths to disk so that we can use then\r\n",
        "# when evaluating/testing our object detector\r\n",
        "print(\"[INFO] saving testing image paths...\")\r\n",
        "f = open(TEST_FILENAMES, \"w\")\r\n",
        "f.write(\"\\n\".join(testPaths))\r\n",
        "f.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] saving testing image paths...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvHIk2UoBXwa"
      },
      "source": [
        "# Building Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEKV1rtBBW8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42087e1c-09f6-4310-a40a-3db1e64fa960"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.applications import MobileNetV2\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Input\r\n",
        "from keras.models import Model\r\n",
        "\r\n",
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\r\n",
        "# left off\r\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\r\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\r\n",
        "\r\n",
        "flatten = baseModel.output\r\n",
        "flatten = Flatten()(flatten)\r\n",
        "\r\n",
        "# construct a fully-connected layer header to output the predicted\r\n",
        "# bounding box coordinates\r\n",
        "bboxHead = Dense(32, activation=\"relu\")(flatten)\r\n",
        "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\r\n",
        "bboxHead = Dense(128, activation=\"relu\")(bboxHead)\r\n",
        "bboxHead = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bboxHead)\r\n",
        "\r\n",
        "# construct a second fully-connected layer head, this one to predict\r\n",
        "# the class label\r\n",
        "softmaxHead = Dense(512, activation=\"relu\")(flatten)\r\n",
        "softmaxHead = Dropout(0.5)(softmaxHead)\r\n",
        "softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\r\n",
        "softmaxHead = Dropout(0.5)(softmaxHead)\r\n",
        "softmaxHead = Dense(3, activation=\"softmax\", name=\"class_label\")(softmaxHead)\r\n",
        "\r\n",
        "# put together our model which accept an input image and then output\r\n",
        "# bounding box coordinates and a class label\r\n",
        "model = Model(\r\n",
        "\tinputs=baseModel.input,\r\n",
        "\toutputs=(bboxHead, softmaxHead)\r\n",
        ")\r\n",
        "\r\n",
        "for layer in baseModel.layers:\r\n",
        "\tlayer.trainable = False\r\n",
        "\r\n",
        "# define a dictionary to set the loss methods -- categorical\r\n",
        "# cross-entropy for the class label head and mean absolute error\r\n",
        "# for the bounding box head\r\n",
        "losses = {\r\n",
        "\t\"class_label\": \"categorical_crossentropy\",\r\n",
        "\t\"bounding_box\": \"mean_squared_error\",\r\n",
        "}\r\n",
        "\r\n",
        "# define a dictionary that specifies the weights per loss (both the\r\n",
        "# class label and bounding box outputs will receive equal weight)\r\n",
        "lossWeights = {\r\n",
        "\t\"class_label\": 1.0,\r\n",
        "\t\"bounding_box\": 1.0\r\n",
        "}\r\n",
        "\r\n",
        "# initialize the optimizer, compile the model, and show the model\r\n",
        "# summary\r\n",
        "model.compile(loss=losses, optimizer='rmsprop', metrics=[\"acc\"], loss_weights=lossWeights)\r\n",
        "\r\n",
        "# construct a dictionary for our target training outputs\r\n",
        "trainTargets = {\r\n",
        "\t\"class_label\": trainLabels,\r\n",
        "\t\"bounding_box\": trainBBoxes\r\n",
        "}\r\n",
        "\r\n",
        "# construct a second dictionary, this one for our target testing\r\n",
        "# outputs\r\n",
        "testTargets = {\r\n",
        "\t\"class_label\": testLabels,\r\n",
        "\t\"bounding_box\": testBBoxes\r\n",
        "}\r\n",
        "\r\n",
        "r\"\"\"\r\n",
        "# Create model structure\r\n",
        "model = keras.Sequential([\r\n",
        "    # Input Layer\r\n",
        "    keras.layers.Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),activation='relu'),\r\n",
        "    keras.layers.MaxPooling2D(),\r\n",
        "\r\n",
        "    # Hidden Layer\r\n",
        "    keras.layers.Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),activation='relu'),\r\n",
        "    keras.layers.MaxPooling2D(),\r\n",
        "    keras.layers.Conv2D(input_shape=(224,224,3),filters=128,kernel_size=(3,3),activation='relu'),\r\n",
        "    keras.layers.MaxPooling2D(),\r\n",
        "    keras.layers.Flatten(),\r\n",
        "    keras.layers.Dense(512),\r\n",
        "    keras.layers.Activation('relu'),\r\n",
        "    \r\n",
        "    # Output Layer\r\n",
        "    keras.layers.Dense(3),\r\n",
        "    keras.layers.Activation('softmax')\r\n",
        "])\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# Compile model\r\n",
        "#model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(),metrics=['acc'])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65lLTRFRjTm3"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIgCoWkZjVVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8871fb65-4a6a-4b7e-ab34-fd3cf57977df"
      },
      "source": [
        "# Create early stopping callback\r\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=3)\r\n",
        "\r\n",
        "# Store model history into variable\r\n",
        "history = model.fit(trainImages, trainTargets,\r\n",
        "                    steps_per_epoch = len(trainImages),\r\n",
        "                    validation_data = (testImages, testTargets),\r\n",
        "                  \tvalidation_steps = len(testImages),\r\n",
        "                    validation_freq=1,\r\n",
        "                    epochs = 50,\r\n",
        "                    #callbacks = [early_stopping],\r\n",
        "                    verbose = 1)\r\n",
        "\r\n",
        "# Plot model training history\r\n",
        "def plot_history():\r\n",
        "    plt.plot(history.history['acc'],label='acc')\r\n",
        "    plt.plot(history.history['val_acc'],label='val_acc')\r\n",
        "    #plt.plot(history.history['loss'],label='loss')\r\n",
        "    #plt.plot(history.history['val_loss'],label='val_loss')\r\n",
        "    plt.legend()\r\n",
        "    plt.title('Training History')\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('value')\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.grid(True)\r\n",
        "    plt.savefig('output/training_history.jpg')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "plot_history()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2652/2652 [==============================] - 66s 21ms/step - loss: 15.6389 - bounding_box_loss: 0.1070 - class_label_loss: 15.5319 - bounding_box_acc: 0.5882 - class_label_acc: 0.6991 - val_loss: 2.9746 - val_bounding_box_loss: 0.0497 - val_class_label_loss: 2.9249 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7741\n",
            "Epoch 2/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.1870 - bounding_box_loss: 0.0511 - class_label_loss: 3.1359 - bounding_box_acc: 0.6173 - class_label_acc: 0.7596 - val_loss: 2.9961 - val_bounding_box_loss: 0.0496 - val_class_label_loss: 2.9465 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7681\n",
            "Epoch 3/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.0815 - bounding_box_loss: 0.0510 - class_label_loss: 3.0304 - bounding_box_acc: 0.6386 - class_label_acc: 0.7765 - val_loss: 2.9675 - val_bounding_box_loss: 0.0495 - val_class_label_loss: 2.9180 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7801\n",
            "Epoch 4/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.8462 - bounding_box_loss: 0.0512 - class_label_loss: 3.7950 - bounding_box_acc: 0.6205 - class_label_acc: 0.7867 - val_loss: 6.0186 - val_bounding_box_loss: 0.0498 - val_class_label_loss: 5.9689 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7816\n",
            "Epoch 5/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 2.9961 - bounding_box_loss: 0.0515 - class_label_loss: 2.9447 - bounding_box_acc: 0.6365 - class_label_acc: 0.8017 - val_loss: 5.8704 - val_bounding_box_loss: 0.0500 - val_class_label_loss: 5.8205 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7801\n",
            "Epoch 6/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.0548 - bounding_box_loss: 0.0517 - class_label_loss: 3.0031 - bounding_box_acc: 0.6309 - class_label_acc: 0.7997 - val_loss: 6.6381 - val_bounding_box_loss: 0.0496 - val_class_label_loss: 6.5885 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7651\n",
            "Epoch 7/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.5008 - bounding_box_loss: 0.0522 - class_label_loss: 3.4485 - bounding_box_acc: 0.6133 - class_label_acc: 0.8093 - val_loss: 10.6601 - val_bounding_box_loss: 0.0496 - val_class_label_loss: 10.6105 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7590\n",
            "Epoch 8/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 2.7635 - bounding_box_loss: 0.0519 - class_label_loss: 2.7116 - bounding_box_acc: 0.6290 - class_label_acc: 0.8022 - val_loss: 9.8180 - val_bounding_box_loss: 0.0496 - val_class_label_loss: 9.7684 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7816\n",
            "Epoch 9/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 2.7565 - bounding_box_loss: 0.0515 - class_label_loss: 2.7050 - bounding_box_acc: 0.6249 - class_label_acc: 0.8206 - val_loss: 6.8539 - val_bounding_box_loss: 0.0496 - val_class_label_loss: 6.8042 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7846\n",
            "Epoch 10/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 2.6474 - bounding_box_loss: 0.0519 - class_label_loss: 2.5956 - bounding_box_acc: 0.6291 - class_label_acc: 0.8234 - val_loss: 10.7250 - val_bounding_box_loss: 0.0495 - val_class_label_loss: 10.6754 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7831\n",
            "Epoch 11/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.0012 - bounding_box_loss: 0.0519 - class_label_loss: 2.9493 - bounding_box_acc: 0.6327 - class_label_acc: 0.8119 - val_loss: 9.2547 - val_bounding_box_loss: 0.0496 - val_class_label_loss: 9.2050 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7861\n",
            "Epoch 12/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.0196 - bounding_box_loss: 0.0514 - class_label_loss: 2.9682 - bounding_box_acc: 0.6258 - class_label_acc: 0.7972 - val_loss: 8.8781 - val_bounding_box_loss: 0.0503 - val_class_label_loss: 8.8278 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7696\n",
            "Epoch 13/50\n",
            "2652/2652 [==============================] - 56s 21ms/step - loss: 3.3659 - bounding_box_loss: 0.0501 - class_label_loss: 3.3158 - bounding_box_acc: 0.6433 - class_label_acc: 0.8121 - val_loss: 10.5209 - val_bounding_box_loss: 0.0496 - val_class_label_loss: 10.4713 - val_bounding_box_acc: 0.6084 - val_class_label_acc: 0.7681\n",
            "Epoch 14/50\n",
            "1551/2652 [================>.............] - ETA: 21s - loss: 2.2863 - bounding_box_loss: 0.0518 - class_label_loss: 2.2345 - bounding_box_acc: 0.6024 - class_label_acc: 0.8101"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}